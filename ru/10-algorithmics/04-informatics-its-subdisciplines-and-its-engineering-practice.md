---
title: Информатика, её поддисциплины и её инженерная практика
---

Дэвид Дойч считает информатику/computer
science^[<https://en.wikipedia.org/wiki/Computer_science>]
одной из четырёх нитей, без которых не получается объяснить структуру
нашей реальности (другие --- это квантовая физика в интерпретации
Эверетта, эволюционная эпистемология Поппера, эволюционная теория
репликаторов Докинза).

**Информатику** как целостное учение часто
делят^[<https://cstheory.stackexchange.com/questions/1521/origins-and-applications-of-theory-a-vs-theory-b>]
на Theory A как **алгоритмику**, занимающуюся вопросами сложности
алгоритмов и их эффективностью, и Theory B, занимающуюся вопросами
теории информации в физике, семантики, теории понятий, онтологии и
логики как «вычислений по правилам». Всё это оказывается
трансдисциплинами, стыкующими наши знания о поведении
математических/идеальных/ментальных объектов в их связи с моделированием
предметов реального мира и представленностью в виде знаков в физическом
мире.

Вот краткое изложение шутки, которая отсылает нас к Theory B --- всегда
помним, что вычисления мы ведём с математическими объектами, которые
что-то отражают в реально мире, и мышление семантиков, типологов,
онтологов, логиков всегда будет про это^[самое ранее
упоминание было в 2019 году ---
<http://chemistry-chemists.com/forum/viewtopic.php?f=45&t=1600&start=100>,
это краткое изложение более длинного и полного рассказа catcha в 2011
году,
<https://www.multitran.com/c/m.exe?a=4&MessNum=257738&l1=1&l2=2>]:

Буратино дали 3 яблока. Два он отдал Мальвине. Сколько яблок у него
осталось?

1\. (initializing) Неизвестно, сколько яблок было у Буратино до того, как
ему дали 3 яблока.

2\. Неизвестно, два «чего» он отдал Мальвине.

3\. Неизвестно, не является ли Буратино и Мальвина двумя разными
сущностями, или одно из них это ссылка на другое.

4\. Неизвестно, что подразумевается под процессом «отдал», и результат
этого процесса (может, Мальвина не взяла).

5\. Непонятно, в какой момент «осталось». После того как он попытался
дать в первый раз?

6\. (state) Неизвестно, не являются ли яблоки частью состояния Буратино,
или самостоятельными объектами.

7\. (волатильность) Не ясно, сколько времени прошло с момента нахождения
яблок у Буратино до процесса «отдачи\":

\- может, яблоки полураспадаются сами по себе (сгнивают), или
возвращаются к тем, кто их дал.

\- может, их едят, пока они у Буратино, а он просто этого не знает.

8\. (длина переменной) Не ясно, сколько яблок может удержать Буратино.

Может, всего 2\... и если он 2 отдал, у него нет яблок.

9\. Если процесс дачи яблок рекурсивный, мы все останемся без яблок.

В интеллект-стеке мы рассматриваем SoTA информатики в разных
трансдисциплинах. В текущем подразделе мы кратко характеризуем
информатику в целом, но всё-таки больше раскрываем алгоритмику. В быту
же IT/информатика встречается не только как научная дисциплина/science
(теория и средства моделирования для теории), но и как опирающаяся на
теорию инженерная практика создания компьютеров как физических
вычислителей, а также программного обеспечения к ним. Грамотный человек
сегодня не только умеет самостоятельно читать сложный текст на
естественном языке и писать слепым десятипальцевым методом на
клавиатуре, но и может отмоделировать какой-то кусок мира и
формализовать его в какой-то мере, и даже выполнить какие-то несложные
вычисления с помощью компьютера --- как предки с трудом научились
умножать «в столбик», как дедушки-бабушки научились это делать при
помощи калькулятора, так любой человек после окончания школы вроде как
должен уметь сделать это на каком-то языке программирования, и уж точно
в системах NoCode/LowCode (productivity tools), то есть программируя на
языке электронных таблиц.

Понятие алгоритма как некоторой последовательности вычислений как
операций над какой-то памятью с данными, при которой из известных
входных данных получаются требуемые выходные данные, все знают ещё из
школы. Сегодня понятно^[первый доклад в
<https://vimeo.com/553810189>, слайды
<https://yadi.sk/i/F6e33aVANX3J3g>], как алгоритмику в
объёме требований по программированию из текущего ЕГЭ дать уже к
окончанию начальной школы, причём детям на это требуется решать задачи
всего около 16 часов (да, так мало!), а взрослым около 8 часов (они
всё-таки более организованы, чем дети). Удивителен сам факт, что дети и
взрослые осваивают азы мастерства императивного/процедурного
программирования за примерно одинаковое время, а для этого достигают
понимания основных понятий: алгоритма, вычислителя/интерпретатора
алгоритма, языка программирования, программиста (составляющего
алгоритм), шага алгоритма, данных. Эта учебная программа уже преподаётся
на русском языке десяткам тысяч
детей^[<https://infomir.ru/news/>].

Но всё-таки компьютеры в массе своей в труде используются только как
ручка-бумажка-на-стероидах. Не «вычислитель» (computer --- это
вычислитель!), а просто «счёты с хорошей коллективной памятью». Память
при этом, конечно, структурированная --- она содержит не просто какие-то
тексты и картинки, а описания мира/модели, обширные «базы данных» с
довольно кучерявыми «моделями данных». Или даже неструктурированную
информацию для полнотекстового поиска в корпоративных хранилищах
документов. Как устроены эти описания мира разной степени формальности,
в computer science разбирается в рамках Theory B (теория информации,
логика, семантика, далее мы будем ещё рассматривать онтологию с
многоуровневым мета-моделированием, они все идут по этой линии Theory
B): как мы думаем о мире, как записываем наши мысли, что означают знаки,
как представлены знаки на физических носителях, как их интерпретировать.
Это важнейшее использование компьютеров на сегодняшний день: личная и
коллективная собранность людей сегодня базируется именно на таком
использовании «компьютера-как-ручки-бумажки», вся алгоритмика в её
разнообразии оказывается не такой уж важной, разве что речь идёт о
задачах масштабирования (когда запомнить, а потом найти надо очень много
разнородных данных, становится важна эффективность алгоритмов, которые
этим занимаются).

Вычислительная мощь компьютеров в бизнесе пока задействуется примерно в
объёме бухгалтерских счётов-на-стероидах, простейшие сложения и
умножения, сравнения текстовых строк в целях их упорядочивания. Это
вычисления человека, который использует компьютеры и компьютерную связь
для помощи в управлении личным и коллективным вниманием, для организации
общения с другими людьми, для умощнения памяти. Вычислять ничего не
нужно, знание о том, как составить простой алгоритм и объяснить его
компьютеру на языке программирования школьники получают, сдают ЕГЭ --- и
дальше не используют. Проблема в том, что в 21 веке алгоритмика (Theory
A из computer science/информатики) поменялась, и нужно с ней разбираться
заново: появились нейросетевые вычисления, которые привели к изменению
архитектуры компьютеров («видеокарты» превратились в GPU как graphics
processing unit, но с появлением алгоритмов искусственного интеллекта их
часто стали называть general processing unit, чтобы подчернуть
нейтральность по отношению к вычислениям именно
графики^[<https://en.wikipedia.org/wiki/Graphics_processing_unit>]),
а ещё появились квантовые компьютеры, оптические
компьютеры^[<https://en.wikipedia.org/wiki/Optical_computing>],
компьютеры на
органоидах^[<https://www.frontiersin.org/journals/science/articles/10.3389/fsci.2023.1017235>].
В предыдущих разделах курса мы уже узнали о существовании ускорителей
для разных видов алгоритмов, ибо по теореме о бесплатных обедах для
универсального компьютера (то есть выполняющего все возможные в природе
вычисления, эквивалентного машине Тьюринга) возможно множество
реализаций, но вот универсальной реализации, дающей большую скорость
вычислений для вообще всех алгоритмов  --- такого нет. Для каждого вида
алгоритмов хорошо бы иметь какой-то ускоритель, что не отменяет
необходимость универсального компьютера, который хоть и плохо (то есть
неэффективно, с затратами большого количества энергии и времени), но
выполняет самые разные классы алгоритмов, а не какой-то один узкий
класс.
